{"cells":[{"cell_type":"code","source":["# Needs 2 nd verison\nfrom pyspark import SparkConf\n# path='/FileStore/tables/dice_com_job_us_sample-3431c.csv'\npath='/FileStore/tables/Newest_Dice.csv'\n\n# path='/FileStore/tables/Newly_Dice.csv'\n\n# Getting the data into spark dataframe\n# Using header = true to skip the header and read the data properly\ndf=sqlContext.read.format(\"csv\")\\\n        .option(\"header\",\"true\")\\\n        .option(\"inferSchmea\",\"true\")\\\n        .load(path)\n\n\n# df = sqlContext.read.format(\"csv\")(path, inferSchema = True, header=True)\nprint df.dtypes\ndf.show(5)\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df.describe()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["    # Basic Stopwords\n    stopwords = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n    u'can', 'cant', 'come', u'could', 'couldnt', u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n    u'each', u'few', 'finally', u'for', u'from', u'further', \n    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n    u'just', u'll',u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n    u'no', u'nor', u'not', u'now',u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n    u'r', u're', u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', u'under', u'until', u'up', u'very', u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import struct\n\ndef data_cleansing(record):\n    text  = record[3]\n    words = text.split()\n    text_out = [word.lower() for word in words if word.lower() not in stopwords]\n    return text_out\n\nudf_dataCleansing = udf(data_cleansing , ArrayType(StringType()))\n\nprocessed_text = df.withColumn(\"words\", udf_dataCleansing(struct([df[x] for x in df.columns])))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["print(processed_text)\nprint(df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.feature import IDF\n# importing inverse document frequency\nfrom pyspark.ml.feature import CountVectorizer\n# Getting all the counts of all the words using the library CountVectorizer\n\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\ncvmodel = cv.fit(processed_text)\n\nvectorData = cvmodel.transform(processed_text)\n\nvocab = cvmodel.vocabulary\nvocab_broadcast = sc.broadcast(vocab)\n \nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(vectorData)\nrescaledData = idfModel.transform(vectorData)\n# show the transformed data\nrescaledData.select('jobdescription','words','rawFeatures','features').show(10)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.clustering import LDA\n\n# The value of k is essentially getting the top 10 trending topics, you can adjust your k accordingly\nlda = LDA(k=10, seed=123, optimizer=\"em\", featuresCol=\"features\")\nmymodel = lda.fit(rescaledData)\n \nldatopics = mymodel.describeTopics()\nldatopics.show(15)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# generate topic summary\ntrending_topics = list()\nnum = 6\n# how many terms you want\nfor row in ldatopics.rdd.map(lambda x: x).collect():\n  topic_id = [row.topic]\n  indices = row.termIndices[:num]\n  topic_weights = row.termWeights\n  terms = [vocab[idx] for idx in indices]\n  trending_topics.append( topic_id + terms )\n\nfor topic in trending_topics:\n  print \"Trensing job postings number \" + str(topic[0]+1) + \"  Terms: \" + str(topic[1:])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"ADBMS_Text_Analytics","notebookId":1123490309392895},"nbformat":4,"nbformat_minor":0}
